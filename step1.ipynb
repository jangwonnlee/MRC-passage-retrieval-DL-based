{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import json\n",
    "from collections import defaultdict\n",
    "\n",
    "#from khaiii import KhaiiiApi\n",
    "#api = KhaiiiApi()\n",
    "\n",
    "import MeCab\n",
    "api = MeCab.Tagger('-d /usr/local/lib/mecab/dic/mecab-ko-dic')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize(anal):\n",
    "    \n",
    "    toks = []\n",
    "    \n",
    "    for word in anal:\n",
    "        for morp in str(word).split('\\t')[1].split('+'):\n",
    "            toks.append(morp.strip())\n",
    "    \n",
    "    return toks\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize2(text):\n",
    "\n",
    "    toks = []\n",
    "\n",
    "    for word in api.parse(text).split('\\n'):\n",
    "        if word == 'EOS':\n",
    "            break\n",
    "        mophs = word.split('\\t')\n",
    "        els = mophs[1].split(',')\n",
    "        if els[7] == '*':\n",
    "            toks.append(mophs[0] + '/' + els[0])\n",
    "        else:\n",
    "            for dels in els[7].split('+'):\n",
    "                dmophs = dels.split('/')\n",
    "                toks.append(dmophs[0] + '/' + dmophs[1])\n",
    "                \n",
    "    return toks\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_text(dic, text):\n",
    "    \n",
    "    for token in tokenize2(text.strip()):\n",
    "        \n",
    "        token = token.strip()\n",
    "        \n",
    "        dic_ori[token] += 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def apply_mincut(frequency_ori, min_freq):\n",
    "    \n",
    "    print('apply minCut and re-generate minCutDic')\n",
    "    mincut_dic = dict(filter(lambda args : args[1] > min_freq, frequency_ori.items()))\n",
    "\n",
    "    return mincut_dic\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "dic_ori = defaultdict(int)\n",
    "\n",
    "num_entry = 0\n",
    "num_context = 0\n",
    "num_question = 0\n",
    "\n",
    "data_set = []\n",
    "entry_set = []\n",
    "\n",
    "with tf.gfile.Open('./KorQuAD_v1.0_train.json', \"r\") as reader:\n",
    "    input_data = json.load(reader)[\"data\"]\n",
    "    \n",
    "    for entry in input_data:\n",
    "        num_entry = num_entry + 1\n",
    "        for paragraph in entry[\"paragraphs\"]:\n",
    "            num_context = num_context + 1\n",
    "            paragraph_text = paragraph[\"context\"]\n",
    "            \n",
    "            read_text(dic_ori, paragraph_text)\n",
    "            \n",
    "            list_ques = []\n",
    "            \n",
    "            for qa in paragraph[\"qas\"]:\n",
    "                num_question = num_question + 1\n",
    "                qas_id = qa[\"id\"]\n",
    "                question_text = qa[\"question\"]\n",
    "                \n",
    "                list_ques.append(question_text)\n",
    "                \n",
    "                read_text(dic_ori, question_text)\n",
    "                \n",
    "                answer = qa[\"answers\"][0]\n",
    "                orig_answer_text = answer[\"text\"]\n",
    "                answer_offset = answer[\"answer_start\"]\n",
    "                \n",
    "            data_set.append( [paragraph_text, list_ques] )\n",
    "            entry_set.append( num_entry - 1 )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "69529\n",
      "1420\n",
      "9681\n",
      "60407\n"
     ]
    }
   ],
   "source": [
    "print(len(dic_ori))\n",
    "print(num_entry)\n",
    "print(num_context)\n",
    "print(num_question)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2010년 8월, 저널 The Lancet Infectious Diseases에 최근 행해진 다국적 연구 결과가 실린 바 있다. 이 연구는 NDM-1 유전자를 가진 박테리아의 발생과 전파를 분석하였다. 연구에 사용된 케이스는 영국 37건, 첸나이 26건, 인도 하리아나 주 26건, 그 외 인도, 파키스탄의 여러 지역 73건 등이었다. 저자의 말에 따르면, 많은 균주들이 NDM-1 유전자를 플라스미드 상에 가지고 있었으며, 이 때문에 균과 균 사이의 유전자 전달 (gene transfer)이 쉽게 가능할 것으로 보았다. 연구 시 다뤄진 모든 균주는 베타-락탐계열 항생제, 퀴놀론 (quinolone) 계열 항생제, 아미노글리코사이드 등 여러 항생제에 내성을 보였으나, 대부분 폴리믹신 (polymyxin) 계열 항생제 콜리스틴 (colistin)에는 감수성을 보였다.\n"
     ]
    }
   ],
   "source": [
    "print(paragraph_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['2010/SN', '년/NNBC', '8/SN', '월/NNBC', ',/SC', '저널/NNG', 'The/SL', 'Lancet/SL', 'Infectious/SL', 'Diseases/SL', '에/JKB', '최근/NNG', '행하/VV', '아/EC', '지/VX', 'ᆫ/ETM', '다/NNG', '국적/NNG', '연구/NNG', '결과/NNG', '가/JKS', '실리/VV', 'ᆫ/ETM', '바/NNB', '있/VV', '다/EF', './SF', '이/MM', '연구/NNG', '는/JX', 'NDM/SL', '-/SY', '1/SN', '유전/NNG', '자/NNG', '를/JKO', '가지/VV', 'ᆫ/ETM', '박테리아/NNG', '의/JKG', '발생/NNG', '과/JC', '전파/NNG', '를/JKO', '분석/NNG', '하/XSV', '였/EP', '다/EF', './SF', '연구/NNG', '에/JKB', '사용/NNG', '되/XSV', 'ᆫ/ETM', '케이스/NNG', '는/JX', '영국/NNP', '37/SN', '건/NNBC', ',/SC', '첸나이/NNP', '26/SN', '건/NNBC', ',/SC', '인도/NNP', '하리아나/NNP', '주/NNG', '26/SN', '건/NNBC', ',/SC', '그/MM', '외/NNB', '인도/NNP', ',/SC', '파키스탄/NNP', '의/JKG', '여러/MM', '지역/NNG', '73/SN', '건/NNBC', '등/NNB', '이/VCP', '었/EP', '다/EF', './SF', '저자/NNG', '의/JKG', '말/NNG', '에/JKB', '따르/VV', '면/EC', ',/SC', '많/VA', '은/ETM', '균주/NNG', '들/XSN', '이/JKS', 'NDM/SL', '-/SY', '1/SN', '유전/NNG', '자/NNG', '를/JKO', '플라스미드/NNG', '상/NNG', '에/JKB', '가지/VV', '고/EC', '있/VX', '었/EP', '으며/EC', ',/SC', '이/MM', '때문/NNB', '에/JKB', '균/NNG', '과/JC', '균/NNG', '사이/NNG', '의/JKG', '유전/NNG', '자/NNG', '전달/NNG', '(/SSO', 'gene/SL', 'transfer/SL', ')/SSC', '이/JKS', '쉽/VA', '게/EC', '가능/NNG', '하/XSV', 'ᆯ/ETM', '것/NNB', '으로/JKB', '보/VV', '았/EP', '다/EF', './SF', '연구/NNG', '시/NNG', '다루/VV', '어/EC', '지/VX', 'ᆫ/ETM', '모든/MM', '균주/NNG', '는/JX', '베타/NNG', '-/SY', '락탐/NNG', '계열/NNG', '항생/NNG', '제/NNG', ',/SC', '퀴놀론/UNKNOWN', '(/SSO', 'quinolone/SL', ')/SSC', '계열/NNG', '항생/NNG', '제/NNG', ',/SC', '아미노글리코사이드/NNG', '등/NNB', '여러/MM', '항생/NNG', '제/NNG', '에/JKB', '내성/NNP', '을/JKO', '보이/VV', '었/EP', '으나/EC', ',/SC', '대/NNG', '부분/NNG', '폴리/NNG', '믹/NNG', '신/NNG', '(/SSO', 'polymyxin/SL', ')/SSC', '계열/NNG', '항생/NNG', '제/NNG', '콜리스틴/NNG', '(/SSO', 'colistin/SL', ')/SSC', '에/JKB', '는/JX', '감수/NNG', '성/XSN', '을/JKO', '보이/VV', '었/EP', '다/EF', './SF']\n"
     ]
    }
   ],
   "source": [
    "print(tokenize2(paragraph_text.strip()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9681\n"
     ]
    }
   ],
   "source": [
    "print(len(entry_set))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_entry = 0\n",
    "num_context = 0\n",
    "num_question = 0\n",
    "\n",
    "with tf.gfile.Open('./ko_wiki_v1_squad.json', \"r\") as reader:\n",
    "    input_data = json.load(reader)[\"data\"]\n",
    "    \n",
    "    for entry in input_data:\n",
    "        num_entry = num_entry + 1\n",
    "        for paragraph in entry[\"paragraphs\"]:\n",
    "            num_context = num_context + 1\n",
    "            paragraph_text = paragraph[\"context\"]\n",
    "            \n",
    "            read_text(dic_ori, paragraph_text)\n",
    "            \n",
    "            list_ques = []\n",
    "            \n",
    "            for qa in paragraph[\"qas\"]:\n",
    "                num_question = num_question + 1\n",
    "                qas_id = qa[\"id\"]\n",
    "                question_text = qa[\"question\"]\n",
    "                \n",
    "                list_ques.append(question_text)\n",
    "                \n",
    "                read_text(dic_ori, question_text)\n",
    "                \n",
    "                answer = qa[\"answers\"][0]\n",
    "                orig_answer_text = answer[\"text\"]\n",
    "                answer_offset = answer[\"answer_start\"]\n",
    "                \n",
    "            data_set.append( [paragraph_text, list_ques] )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "148261\n",
      "68538\n",
      "68538\n",
      "100268\n"
     ]
    }
   ],
   "source": [
    "print(len(dic_ori))\n",
    "print(num_entry)\n",
    "print(num_context)\n",
    "print(num_question)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_entry = 0\n",
    "num_context = 0\n",
    "num_question = 0\n",
    "\n",
    "data_set_val = []\n",
    "\n",
    "with tf.gfile.Open('./KorQuAD_v1.0_dev.json', \"r\") as reader:\n",
    "    input_data = json.load(reader)[\"data\"]\n",
    "    \n",
    "    for entry in input_data:\n",
    "        num_entry = num_entry + 1\n",
    "        for paragraph in entry[\"paragraphs\"]:\n",
    "            num_context = num_context + 1\n",
    "            paragraph_text = paragraph[\"context\"]\n",
    "            \n",
    "            read_text(dic_ori, paragraph_text)\n",
    "            \n",
    "            list_ques = []\n",
    "            \n",
    "            for qa in paragraph[\"qas\"]:\n",
    "                num_question = num_question + 1\n",
    "                qas_id = qa[\"id\"]\n",
    "                question_text = qa[\"question\"]\n",
    "                \n",
    "                list_ques.append(question_text)\n",
    "                \n",
    "                read_text(dic_ori, question_text)\n",
    "                \n",
    "                answer = qa[\"answers\"][0]\n",
    "                orig_answer_text = answer[\"text\"]\n",
    "                answer_offset = answer[\"answer_start\"]\n",
    "                \n",
    "            data_set_val.append( [paragraph_text, list_ques] )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "149722\n",
      "140\n",
      "964\n",
      "5774\n"
     ]
    }
   ],
   "source": [
    "print(len(dic_ori))\n",
    "print(num_entry)\n",
    "print(num_context)\n",
    "print(num_question)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "149722\n"
     ]
    }
   ],
   "source": [
    "#dic = apply_mincut(dic_ori, 3)\n",
    "dic = dic_ori\n",
    "print(len(dic))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "149724\n"
     ]
    }
   ],
   "source": [
    "voca = {}\n",
    "\n",
    "voca['_PAD_'] = len(voca)\n",
    "voca['_UNK_'] = len(voca)\n",
    "\n",
    "for word in dic.keys():\n",
    "    voca[word] = len(voca)    \n",
    "    \n",
    "print(len(voca))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "pickle.dump( voca, open('./v1_dic.pkl', 'wb')  )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "149724\n"
     ]
    }
   ],
   "source": [
    "dic = pickle.load(open('./v1_dic.pkl', 'rb') )\n",
    "print(len(dic))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "78219\n",
      "160675\n"
     ]
    }
   ],
   "source": [
    "num_dt = 0\n",
    "num_dt2 = 0\n",
    "\n",
    "for data in data_set:\n",
    "    num_dt = num_dt + 1\n",
    "    for data2 in data[1]:\n",
    "        num_dt2 = num_dt2 + 1\n",
    "\n",
    "print(num_dt)\n",
    "print(num_dt2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "964\n",
      "5774\n"
     ]
    }
   ],
   "source": [
    "num_dt = 0\n",
    "num_dt2 = 0\n",
    "\n",
    "for data in data_set_val:\n",
    "    num_dt = num_dt + 1\n",
    "    for data2 in data[1]:\n",
    "        num_dt2 = num_dt2 + 1\n",
    "\n",
    "print(num_dt)\n",
    "print(num_dt2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['1839년 바그너는 괴테의 파우스트을 처음 읽고 그 내용에 마음이 끌려 이를 소재로 해서 하나의 교향곡을 쓰려는 뜻을 갖는다. 이 시기 바그너는 1838년에 빛 독촉으로 산전수전을 다 걲은 상황이라 좌절과 실망에 가득했으며 메피스토펠레스를 만나는 파우스트의 심경에 공감했다고 한다. 또한 파리에서 아브네크의 지휘로 파리 음악원 관현악단이 연주하는 베토벤의 교향곡 9번을 듣고 깊은 감명을 받았는데, 이것이 이듬해 1월에 파우스트의 서곡으로 쓰여진 이 작품에 조금이라도 영향을 끼쳤으리라는 것은 의심할 여지가 없다. 여기의 라단조 조성의 경우에도 그의 전기에 적혀 있는 것처럼 단순한 정신적 피로나 실의가 반영된 것이 아니라 베토벤의 합창교향곡 조성의 영향을 받은 것을 볼 수 있다. 그렇게 교향곡 작곡을 1839년부터 40년에 걸쳐 파리에서 착수했으나 1악장을 쓴 뒤에 중단했다. 또한 작품의 완성과 동시에 그는 이 서곡(1악장)을 파리 음악원의 연주회에서 연주할 파트보까지 준비하였으나, 실제로는 이루어지지는 않았다. 결국 초연은 4년 반이 지난 후에 드레스덴에서 연주되었고 재연도 이루어졌지만, 이후에 그대로 방치되고 말았다. 그 사이에 그는 리엔치와 방황하는 네덜란드인을 완성하고 탄호이저에도 착수하는 등 분주한 시간을 보냈는데, 그런 바쁜 생활이 이 곡을 잊게 한 것이 아닌가 하는 의견도 있다.', ['바그너는 괴테의 파우스트를 읽고 무엇을 쓰고자 했는가?', '바그너는 교향곡 작곡을 어디까지 쓴 뒤에 중단했는가?', '바그너가 파우스트 서곡을 쓸 때 어떤 곡의 영향을 받았는가?', '1839년 바그너가 교향곡의 소재로 쓰려고 했던 책은?', '파우스트 서곡의 라단조 조성이 영향을 받은 베토벤의 곡은?', '바그너가 파우스트를 처음으로 읽은 년도는?', '바그너가 처음 교향곡 작곡을 한 장소는?', '바그너의 1악장의 초연은 어디서 연주되었는가?']]\n"
     ]
    }
   ],
   "source": [
    "print(data_set[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1839년 바그너는 괴테의 파우스트을 처음 읽고 그 내용에 마음이 끌려 이를 소재로 해서 하나의 교향곡을 쓰려는 뜻을 갖는다. 이 시기 바그너는 1838년에 빛 독촉으로 산전수전을 다 걲은 상황이라 좌절과 실망에 가득했으며 메피스토펠레스를 만나는 파우스트의 심경에 공감했다고 한다. 또한 파리에서 아브네크의 지휘로 파리 음악원 관현악단이 연주하는 베토벤의 교향곡 9번을 듣고 깊은 감명을 받았는데, 이것이 이듬해 1월에 파우스트의 서곡으로 쓰여진 이 작품에 조금이라도 영향을 끼쳤으리라는 것은 의심할 여지가 없다. 여기의 라단조 조성의 경우에도 그의 전기에 적혀 있는 것처럼 단순한 정신적 피로나 실의가 반영된 것이 아니라 베토벤의 합창교향곡 조성의 영향을 받은 것을 볼 수 있다. 그렇게 교향곡 작곡을 1839년부터 40년에 걸쳐 파리에서 착수했으나 1악장을 쓴 뒤에 중단했다. 또한 작품의 완성과 동시에 그는 이 서곡(1악장)을 파리 음악원의 연주회에서 연주할 파트보까지 준비하였으나, 실제로는 이루어지지는 않았다. 결국 초연은 4년 반이 지난 후에 드레스덴에서 연주되었고 재연도 이루어졌지만, 이후에 그대로 방치되고 말았다. 그 사이에 그는 리엔치와 방황하는 네덜란드인을 완성하고 탄호이저에도 착수하는 등 분주한 시간을 보냈는데, 그런 바쁜 생활이 이 곡을 잊게 한 것이 아닌가 하는 의견도 있다.\n"
     ]
    }
   ],
   "source": [
    "print(data_set[0][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['바그너는 괴테의 파우스트를 읽고 무엇을 쓰고자 했는가?', '바그너는 교향곡 작곡을 어디까지 쓴 뒤에 중단했는가?', '바그너가 파우스트 서곡을 쓸 때 어떤 곡의 영향을 받았는가?', '1839년 바그너가 교향곡의 소재로 쓰려고 했던 책은?', '파우스트 서곡의 라단조 조성이 영향을 받은 베토벤의 곡은?', '바그너가 파우스트를 처음으로 읽은 년도는?', '바그너가 처음 교향곡 작곡을 한 장소는?', '바그너의 1악장의 초연은 어디서 연주되었는가?']\n"
     ]
    }
   ],
   "source": [
    "print(data_set[0][1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['계속해서 포기하지 않고 싸우던 미후네 돌핀스는 마침내 요코하마 리틀에게서 승리를 쟁취하지만 승리함과 동시에 기권하였기 때문에 전국대회는 나가지 못한다. 계절의 끝, 고로의 의붓 엄마가 시게노 히데키와 결혼하고 시게노가 마린스타즈에서 이적했기 때문에 새로운 가족은 요코하마를 떠나게 되었다. 그리고 고로는 팀원들에게 언젠가는 돌아오겠다는 말만 남긴채 후쿠오카로 가게 된다. 그의 양아버지가 다시 블루오션스로 트레이드되었을 때 다시 미후네로 돌아오고 리틀 야구 시절 친구들이 자라나던 미후네 동중학교로 전학했다. 그러나, 그가 축구부에 들어가서 축구를 하고 야구부에 들어가서 야구를 하지 않자 친구들은 놀라워하지만 나중에 후쿠오카 리틀리그에서 오른쪽 어깨에 부상을 입은 뒤로 야구부에 들어가지 않았다는 사실을 알고 이해한다. 고로는 야구부를 살리려는 코모리를 위협하는 불량 부원의 마음을 바꿔 놓고 함께 중학교 야구부를 재건한다. 팀은 지역 토너먼트에서 강한 실력을 발휘하고 4강에서 친구이자 라이벌은 사토 토시야가 있는 토모노우라 중학교 야구부를 상대로 승리하는 등 지방대회에서 우승하게 된다.', ['메이저에서 고로가 미후네로 돌아와서 야구부가 아닌 축구부에 들어간 이유가 뭐야']]\n"
     ]
    }
   ],
   "source": [
    "print(data_set[78218])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "계속해서 포기하지 않고 싸우던 미후네 돌핀스는 마침내 요코하마 리틀에게서 승리를 쟁취하지만 승리함과 동시에 기권하였기 때문에 전국대회는 나가지 못한다. 계절의 끝, 고로의 의붓 엄마가 시게노 히데키와 결혼하고 시게노가 마린스타즈에서 이적했기 때문에 새로운 가족은 요코하마를 떠나게 되었다. 그리고 고로는 팀원들에게 언젠가는 돌아오겠다는 말만 남긴채 후쿠오카로 가게 된다. 그의 양아버지가 다시 블루오션스로 트레이드되었을 때 다시 미후네로 돌아오고 리틀 야구 시절 친구들이 자라나던 미후네 동중학교로 전학했다. 그러나, 그가 축구부에 들어가서 축구를 하고 야구부에 들어가서 야구를 하지 않자 친구들은 놀라워하지만 나중에 후쿠오카 리틀리그에서 오른쪽 어깨에 부상을 입은 뒤로 야구부에 들어가지 않았다는 사실을 알고 이해한다. 고로는 야구부를 살리려는 코모리를 위협하는 불량 부원의 마음을 바꿔 놓고 함께 중학교 야구부를 재건한다. 팀은 지역 토너먼트에서 강한 실력을 발휘하고 4강에서 친구이자 라이벌은 사토 토시야가 있는 토모노우라 중학교 야구부를 상대로 승리하는 등 지방대회에서 우승하게 된다.\n"
     ]
    }
   ],
   "source": [
    "print(data_set[78218][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['메이저에서 고로가 미후네로 돌아와서 야구부가 아닌 축구부에 들어간 이유가 뭐야']\n"
     ]
    }
   ],
   "source": [
    "print(data_set[78218][1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['1989년 2월 15일 여의도 농민 폭력 시위를 주도한 혐의(폭력행위등처벌에관한법률위반)으로 지명수배되었다. 1989년 3월 12일 서울지방검찰청 공안부는 임종석의 사전구속영장을 발부받았다. 같은 해 6월 30일 평양축전에 임수경을 대표로 파견하여 국가보안법위반 혐의가 추가되었다. 경찰은 12월 18일~20일 사이 서울 경희대학교에서 임종석이 성명 발표를 추진하고 있다는 첩보를 입수했고, 12월 18일 오전 7시 40분 경 가스총과 전자봉으로 무장한 특공조 및 대공과 직원 12명 등 22명의 사복 경찰을 승용차 8대에 나누어 경희대학교에 투입했다. 1989년 12월 18일 오전 8시 15분 경 서울청량리경찰서는 호위 학생 5명과 함께 경희대학교 학생회관 건물 계단을 내려오는 임종석을 발견, 검거해 구속을 집행했다. 임종석은 청량리경찰서에서 약 1시간 동안 조사를 받은 뒤 오전 9시 50분 경 서울 장안동의 서울지방경찰청 공안분실로 인계되었다.', ['임종석이 여의도 농민 폭력 시위를 주도한 혐의로 지명수배 된 날은?', '1989년 6월 30일 평양축전에 대표로 파견 된 인물은?', '임종석이 여의도 농민 폭력 시위를 주도한 혐의로 지명수배된 연도는?', '임종석을 검거한 장소는 경희대 내 어디인가?', '임종석이 조사를 받은 뒤 인계된 곳은 어딘가?', '1989년 2월 15일 여의도 농민 폭력 시위를 주도한 혐의로 지명수배된 사람의 이름은?', '임종석이 1989년 2월 15일에 지명수배 받은 혐의는 어떤 시위를 주도했다는 것인가?']]\n"
     ]
    }
   ],
   "source": [
    "print(data_set_val[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1989년 2월 15일 여의도 농민 폭력 시위를 주도한 혐의(폭력행위등처벌에관한법률위반)으로 지명수배되었다. 1989년 3월 12일 서울지방검찰청 공안부는 임종석의 사전구속영장을 발부받았다. 같은 해 6월 30일 평양축전에 임수경을 대표로 파견하여 국가보안법위반 혐의가 추가되었다. 경찰은 12월 18일~20일 사이 서울 경희대학교에서 임종석이 성명 발표를 추진하고 있다는 첩보를 입수했고, 12월 18일 오전 7시 40분 경 가스총과 전자봉으로 무장한 특공조 및 대공과 직원 12명 등 22명의 사복 경찰을 승용차 8대에 나누어 경희대학교에 투입했다. 1989년 12월 18일 오전 8시 15분 경 서울청량리경찰서는 호위 학생 5명과 함께 경희대학교 학생회관 건물 계단을 내려오는 임종석을 발견, 검거해 구속을 집행했다. 임종석은 청량리경찰서에서 약 1시간 동안 조사를 받은 뒤 오전 9시 50분 경 서울 장안동의 서울지방경찰청 공안분실로 인계되었다.\n"
     ]
    }
   ],
   "source": [
    "print(data_set_val[0][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['임종석이 여의도 농민 폭력 시위를 주도한 혐의로 지명수배 된 날은?', '1989년 6월 30일 평양축전에 대표로 파견 된 인물은?', '임종석이 여의도 농민 폭력 시위를 주도한 혐의로 지명수배된 연도는?', '임종석을 검거한 장소는 경희대 내 어디인가?', '임종석이 조사를 받은 뒤 인계된 곳은 어딘가?', '1989년 2월 15일 여의도 농민 폭력 시위를 주도한 혐의로 지명수배된 사람의 이름은?', '임종석이 1989년 2월 15일에 지명수배 받은 혐의는 어떤 시위를 주도했다는 것인가?']\n"
     ]
    }
   ],
   "source": [
    "print(data_set_val[0][1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_index(word, dic):\n",
    "    \n",
    "    word = word.strip()\n",
    "    \n",
    "    if word in dic:\n",
    "        return dic[word]\n",
    "    else:\n",
    "        return dic['_UNK_']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def return_anti(cur, shuff, max_cnt):\n",
    "    \n",
    "    data_anti = []\n",
    "    num_anti = 0\n",
    "    \n",
    "    for idx in shuff:\n",
    "        if idx == cur:\n",
    "            continue\n",
    "        for res in data_set[idx][1]:\n",
    "            num_anti = num_anti + 1\n",
    "            data_anti.append(res)\n",
    "            \n",
    "            if num_anti == max_cnt:\n",
    "                return data_anti\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def return_anti2(cur, shuff):\n",
    "    \n",
    "    data_anti = []\n",
    "    num_anti = 0\n",
    "    \n",
    "    for idx in shuff:\n",
    "        if idx == cur:\n",
    "            continue\n",
    "        for res in data_set_val[idx][1]:\n",
    "            num_anti = num_anti + 1\n",
    "            data_anti.append(res)\n",
    "            \n",
    "            if num_anti == 10:\n",
    "                return data_anti\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "78219\n"
     ]
    }
   ],
   "source": [
    "from random import shuffle\n",
    "\n",
    "train_set = {}\n",
    "context = []\n",
    "response = []\n",
    "label = []\n",
    "\n",
    "cnt = 0\n",
    "for idx in range(len(data_set)):\n",
    "    \n",
    "    num_res = 0\n",
    "    \n",
    "    for data_res in data_set[idx][1]:\n",
    "        response.append( [find_index(x, dic) for x in tokenize2(data_res.strip())] )\n",
    "        num_res = num_res + 1\n",
    "    \n",
    "    if idx < 9681:\n",
    "        \n",
    "        cores_x = [i for i, e in enumerate(entry_set) if e == entry_set[idx]]\n",
    "        \n",
    "        cores_x.remove(idx)\n",
    "        \n",
    "        if len(cores_x) == 0:\n",
    "            \n",
    "            rand_x = [i for i in range(len(data_set))]\n",
    "            \n",
    "            shuffle(rand_x)\n",
    "            \n",
    "            data_anti = return_anti(idx, rand_x, 10)\n",
    "            \n",
    "            for data_res in data_anti:\n",
    "                response.append( [find_index(x, dic) for x in tokenize2(data_res.strip())] )\n",
    "                \n",
    "        else:\n",
    "            \n",
    "            num_cores_x = 0\n",
    "            \n",
    "            for xx in cores_x:\n",
    "                num_cores_x = num_cores_x + len(data_set[xx][1])\n",
    "                \n",
    "            if num_cores_x < 5:\n",
    "                '''\n",
    "                shuffle(cores_x)\n",
    "                \n",
    "                data_anti = return_anti(idx, cores_x, num_cores_x)\n",
    "                \n",
    "                for data_res in data_anti:\n",
    "                    response.append( [find_index(x, dic) for x in tokenize2(data_res.strip())] )\n",
    "                '''\n",
    "                rand_x = [i for i in range(len(data_set)) if i not in cores_x]\n",
    "                \n",
    "                shuffle(rand_x)\n",
    "                \n",
    "                data_anti = return_anti(idx, rand_x, 10)\n",
    "                \n",
    "                for data_res in data_anti:\n",
    "                    response.append( [find_index(x, dic) for x in tokenize2(data_res.strip())] )\n",
    "                \n",
    "            else:\n",
    "                \n",
    "                shuffle(cores_x)\n",
    "                \n",
    "                data_anti = return_anti(idx, cores_x, 5)\n",
    "                \n",
    "                for data_res in data_anti:\n",
    "                    response.append( [find_index(x, dic) for x in tokenize2(data_res.strip())] )\n",
    "                    \n",
    "                rand_x = [i for i in range(len(data_set)) if i not in cores_x]\n",
    "                \n",
    "                shuffle(rand_x)\n",
    "                \n",
    "                data_anti = return_anti(idx, rand_x, 5)\n",
    "                \n",
    "                for data_res in data_anti:\n",
    "                    response.append( [find_index(x, dic) for x in tokenize2(data_res.strip())] )\n",
    "                    \n",
    "    else:\n",
    "        \n",
    "        rand_x = [i for i in range(len(data_set))]\n",
    "    \n",
    "        shuffle(rand_x)\n",
    "    \n",
    "        data_anti = return_anti(idx, rand_x, 10)\n",
    "    \n",
    "        for data_res in data_anti:\n",
    "            response.append( [find_index(x, dic) for x in tokenize2(data_res.strip())] )\n",
    "        \n",
    "    for idx2 in range(num_res):\n",
    "        label.append(1)\n",
    "        \n",
    "    for idx2 in range(10):\n",
    "        label.append(0)\n",
    "    \n",
    "    cont_tmp = [find_index(x, dic) for x in tokenize2(data_set[idx][0].strip())]\n",
    "    for idx2 in range(num_res+10):\n",
    "        context.append(cont_tmp)\n",
    "    \n",
    "    #print(cnt)\n",
    "    cnt = cnt + 1\n",
    "\n",
    "print(cnt)\n",
    "\n",
    "train_set['c'] = context\n",
    "train_set['r'] = response\n",
    "train_set['y'] = label\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "964\n"
     ]
    }
   ],
   "source": [
    "valid_set = {}\n",
    "context = []\n",
    "response = []\n",
    "label = []\n",
    "\n",
    "log_context = []\n",
    "log_response = []\n",
    "\n",
    "cnt = 0\n",
    "for idx in range(len(data_set_val)):\n",
    "    \n",
    "    num_res = 0\n",
    "    \n",
    "    for data_res in data_set_val[idx][1]:\n",
    "        response.append( [find_index(x, dic) for x in tokenize2(data_res.strip())] )\n",
    "        log_response.append(data_res.strip())\n",
    "        num_res = num_res + 1\n",
    "    \n",
    "    rand_x = [i for i in range(len(data_set_val))]\n",
    "    shuffle(rand_x)\n",
    "    \n",
    "    data_anti = return_anti2(idx, rand_x)\n",
    "    \n",
    "    for data_res in data_anti:\n",
    "        response.append( [find_index(x, dic) for x in tokenize2(data_res.strip())] )\n",
    "        log_response.append(data_res.strip())\n",
    "        \n",
    "    for idx2 in range(num_res):\n",
    "        label.append(1)\n",
    "        \n",
    "    for idx2 in range(10):\n",
    "        label.append(0)\n",
    "    \n",
    "    cont_tmp = [find_index(x, dic) for x in tokenize2(data_set_val[idx][0].strip())]\n",
    "    for idx2 in range(num_res+10):\n",
    "        context.append(cont_tmp)\n",
    "        log_context.append(data_set_val[idx][0].strip())\n",
    "    \n",
    "    #print(idx)\n",
    "    cnt = cnt + 1\n",
    "\n",
    "print(cnt)\n",
    "\n",
    "valid_set['c'] = context\n",
    "valid_set['r'] = response\n",
    "valid_set['y'] = label\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "15414\n",
      "15414\n"
     ]
    }
   ],
   "source": [
    "print(len(log_context))\n",
    "print(len(log_response))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "with open('log_val.csv', 'w', encoding='utf-8', newline='') as ft:\n",
    "    wr = csv.writer(ft)\n",
    "    for idx in range(len(log_context)):\n",
    "        filtered_context = log_context[idx].replace('\\n', '')\n",
    "        filtered_response = log_response[idx].replace('\\n', '')\n",
    "        wr.writerow([filtered_context, filtered_response])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['임종석이 여의도 농민 폭력 시위를 주도한 혐의로 지명수배 된 날은?', '1989년 6월 30일 평양축전에 대표로 파견 된 인물은?', '임종석이 여의도 농민 폭력 시위를 주도한 혐의로 지명수배된 연도는?', '임종석을 검거한 장소는 경희대 내 어디인가?', '임종석이 조사를 받은 뒤 인계된 곳은 어딘가?', '1989년 2월 15일 여의도 농민 폭력 시위를 주도한 혐의로 지명수배된 사람의 이름은?', '임종석이 1989년 2월 15일에 지명수배 받은 혐의는 어떤 시위를 주도했다는 것인가?', '제칠일안식교에서 비롯된 의사과학의 한 종류인 유사지질학의 이름은 무엇인가?', '극보수주의계열의 기독교이자 아직도 노아의 홍수가 있었다고 주장하는 곳은 어디인가?', '고대사회에서 성경은 교리를 다루는 책일 뿐만 아니라 어떤 책으로도 권위가 상당했는가?']\n",
      "10\n"
     ]
    }
   ],
   "source": [
    "print(data_anti)\n",
    "print(len(data_anti))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "942865\n",
      "942865\n",
      "942865\n"
     ]
    }
   ],
   "source": [
    "print(len(train_set['c']))\n",
    "print(len(train_set['r']))\n",
    "print(len(train_set['y']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "15414\n",
      "15414\n",
      "15414\n"
     ]
    }
   ],
   "source": [
    "print(len(valid_set['c']))\n",
    "print(len(valid_set['r']))\n",
    "print(len(valid_set['y']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "pickle.dump( [train_set, valid_set], open('./v1_data.pkl', 'wb') )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
