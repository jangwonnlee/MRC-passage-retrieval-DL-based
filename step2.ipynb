{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import random\n",
    "import pickle\n",
    "\n",
    "import os\n",
    "import time\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_DIR = './'\n",
    "\n",
    "DATA_FILE_NAME = 'v1_data.pkl'\n",
    "#TEST_DATA_FILE_NAME = 'v1_data_test.pkl'  # contains only 500 samples\n",
    "\n",
    "VOCA_FILE_NAME = 'v1_dic.pkl'\n",
    "#GLOVE_FILE_NAME = 'v2_glove.pkl'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ProcessData:\n",
    "    \n",
    "    def __init__(self, is_test):\n",
    "        \n",
    "        print('IS_TEST = {}'.format(str(is_test)))\n",
    "        \n",
    "        self.is_test = is_test\n",
    "        self.voca = None\n",
    "        self.pad_index = 0\n",
    "        self.index2word = {}\n",
    "        \n",
    "        self.train_set = []\n",
    "        self.valid_set = []\n",
    "        \n",
    "        self.load_data()\n",
    "        self.create_train_set()\n",
    "        self.create_valid_set()\n",
    "        \n",
    "        \n",
    "    def load_data(self):\n",
    "        \n",
    "        if self.is_test:\n",
    "            self.train_data, self.valid_data, self.test_data = pickle.load(open(DATA_DIR + TEST_DATA_FILE_NAME, 'r'))\n",
    "            #print 'load data : ' + TEST_DATA_FILE_NAME\n",
    "        else:\n",
    "            self.train_data, self.valid_data = pickle.load(open(DATA_DIR + DATA_FILE_NAME, 'rb'))\n",
    "            #print 'load data : ' + DATA_FILE_NAME\n",
    "        \n",
    "        self.voca = pickle.load(open(DATA_DIR + VOCA_FILE_NAME, 'rb') )\n",
    "        #self.W_glove_init = pickle.load(open(DATA_DIR + GLOVE_FILE_NAME, 'r') )\n",
    "        \n",
    "        self.pad_index = self.voca['_PAD_']\n",
    "        \n",
    "        for w in self.voca:\n",
    "            self.index2word[self.voca[w]] = w\n",
    "        \n",
    "        #print '[completed] load data'\n",
    "        print(\"voca size (include _PAD_, _UNK_): {}\".format ( str( len(self.voca)) ) )\n",
    "        \n",
    "        \n",
    "    # create train set : context -> split by using '__EOS__' -> multiple sentneces\n",
    "    # convert to soucre, target, label\n",
    "    def create_train_set(self):\n",
    "        \n",
    "        data_len = len(self.train_data['c'])\n",
    "        \n",
    "        for index in range(data_len):\n",
    "            \n",
    "            turn =[x.strip() for x in (' '.join(str(e) for e in self.train_data['c'][index])).split(str(self.voca['./SF']))]\n",
    "            turn = [ x for x in turn if len(x) >1]\n",
    "            \n",
    "            tmp_ids = [x.split(' ') for x in turn]\n",
    "            source_ids = []\n",
    "            for sent in tmp_ids:\n",
    "                source_ids.append( [ int(x) for x in sent]  )\n",
    "                \n",
    "            target_ids = self.train_data['r'][index]\n",
    "            label = float(self.train_data['y'][index])\n",
    "            \n",
    "            self.train_set.append( [source_ids, target_ids, label] )\n",
    "        \n",
    "        print(\"[completed] create train set : {}\".format( str(len(self.train_set)) ) )\n",
    "        \n",
    "        \n",
    "    # create valid set : context -> split by using '__EOS__' -> multiple sentneces\n",
    "    # convert to soucre, target, label\n",
    "    def create_valid_set(self):\n",
    "        \n",
    "        data_len = len(self.valid_data['c'])\n",
    "        \n",
    "        for index in range(data_len):\n",
    "            \n",
    "            turn =[x.strip() for x in (' '.join(str(e) for e in self.valid_data['c'][index])).split(str(self.voca['./SF']))]\n",
    "            turn = [ x for x in turn if len(x) >1]\n",
    "            \n",
    "            tmp_ids = [x.split(' ') for x in turn]\n",
    "            source_ids = []\n",
    "            for sent in tmp_ids:\n",
    "                source_ids.append( [ int(x) for x in sent]  )\n",
    "                \n",
    "            target_ids = self.valid_data['r'][index]\n",
    "            label = float(self.valid_data['y'][index])\n",
    "            \n",
    "            self.valid_set.append( [source_ids, target_ids, label] )\n",
    "        \n",
    "        print(\"[completed] create valid set : {}\".format( str(len(self.valid_set)) ) )\n",
    "        \n",
    "        \n",
    "    def get_batch(self, data, batch_size, encoder_size, context_size, encoderR_size, is_test, start_index=0, target_index=1):\n",
    "\n",
    "        encoder_inputs, encoderR_inputs, encoder_seq, context_seq, encoderR_seq, target_labels = [], [], [], [], [], []\n",
    "        index = start_index\n",
    "        \n",
    "        # Get a random batch of encoder and encoderR inputs from data,\n",
    "        # pad them if needed\n",
    "\n",
    "        for _ in range(batch_size):\n",
    "\n",
    "            if is_test is False:\n",
    "                list_encoder_input, encoderR_input, target_label = random.choice(data)\n",
    "            else:\n",
    "                list_encoder_input = data[index][0]\n",
    "                #encoderR_input = data[index][1][target_index]\n",
    "                encoderR_input = data[index][1]\n",
    "                #index = index +1\n",
    "    \n",
    "            list_len = len( list_encoder_input )\n",
    "            tmp_encoder_inputs = []\n",
    "            tmp_encoder_seq = []\n",
    "            \n",
    "            for en_input in list_encoder_input:\n",
    "                encoder_pad = [self.pad_index] * (encoder_size - len( en_input ))\n",
    "                tmp_encoder_inputs.append( (en_input + encoder_pad)[:encoder_size] )        \n",
    "                tmp_encoder_seq.append( min( len( en_input ), encoder_size ) )    \n",
    "            \n",
    "            # add pad\n",
    "            for i in range( context_size - list_len ):\n",
    "                encoder_pad = [self.pad_index] * (encoder_size)\n",
    "                tmp_encoder_inputs.append( encoder_pad )\n",
    "                tmp_encoder_seq.append( 0 ) \n",
    "\n",
    "            encoder_inputs.extend( tmp_encoder_inputs[-context_size:] )\n",
    "            encoder_seq.extend( tmp_encoder_seq[-context_size:] )\n",
    "            \n",
    "            context_seq.append( min(  len(list_encoder_input), context_size  ) )\n",
    "\n",
    "            # encoderR inputs are padded\n",
    "            encoderR_pad = [self.pad_index] * (encoderR_size - len(encoderR_input))\n",
    "            encoderR_inputs.append( (encoderR_input + encoderR_pad)[:encoderR_size]) \n",
    "\n",
    "            encoderR_seq.append( min(len(encoderR_input), encoderR_size) )\n",
    "\n",
    "            # Target Label for batch\n",
    "            if is_test is False:\n",
    "                target_labels.append( int(target_label) )\n",
    "            else:\n",
    "                target_labels.append( int(data[index][2]) )\n",
    "                index = index + 1\n",
    "                #if target_index is 0:\n",
    "                #    target_labels.append( int(1) )\n",
    "                #else:\n",
    "                #    target_labels.append( int(0) )\n",
    "                    \n",
    "                    \n",
    "        return encoder_inputs, encoderR_inputs, encoder_seq, context_seq, encoderR_seq, np.reshape(target_labels, (batch_size, 1))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_dir(dir_name):\n",
    "    if not os.path.isdir(dir_name):\n",
    "        os.mkdir(dir_name)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size=256\n",
    "encoder_size=80\n",
    "context_size=15\n",
    "encoderR_size=160\n",
    "\n",
    "# siaseme RNN\n",
    "num_layer=1\n",
    "hidden_dim=300\n",
    "\n",
    "# context RNN\n",
    "num_layer_con=1\n",
    "hidden_dim_con=300\n",
    "\n",
    "embed_size=300\n",
    "num_train_steps=100000\n",
    "lr=0.001\n",
    "valid_freq=500\n",
    "is_save=1\n",
    "graph_prefix='HRDE_LTC_korquad_v1_'\n",
    "\n",
    "is_test=0\n",
    "use_glove=0\n",
    "\n",
    "dr=0.3\n",
    "dr_con=1.0\n",
    "memory_dr=0.8\n",
    "\n",
    "# latent topic\n",
    "memory_dim=256\n",
    "topic_size=3\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "graph_dir_name = graph_prefix + \\\n",
    "                '_b' + str(batch_size) + \\\n",
    "                '_es' + str(encoder_size) + \\\n",
    "                '_eRs' + str(encoderR_size) + \\\n",
    "                '_cs' + str(context_size) + \\\n",
    "                '_L' + str(num_layer) + \\\n",
    "                '_H' + str(hidden_dim) + \\\n",
    "                '_Lc' + str(num_layer_con) + \\\n",
    "                '_Hc' + str(hidden_dim_con) + \\\n",
    "                '_G' + str(use_glove) + \\\n",
    "                '_dr' + str(dr)  + \\\n",
    "                '_drc' + str(dr_con) + \\\n",
    "                '_drM' + str(memory_dr) + \\\n",
    "                '_M' + str(memory_dim) + \\\n",
    "                '_T' + str(topic_size)\n",
    "\n",
    "if is_save is 1:\n",
    "    create_dir('save/')\n",
    "    create_dir('save/'+ graph_dir_name )\n",
    "\n",
    "create_dir('graph/')\n",
    "create_dir('graph/' + graph_dir_name )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_gen = ProcessData(is_test=is_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from HRDE_Model_mem_v1 import *\n",
    "from HRDE_evaluation import *\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = HRDualEncoderModel(voca_size=len(batch_gen.voca),\n",
    "                               batch_size=batch_size,\n",
    "                               encoder_size=encoder_size,\n",
    "                               context_size=context_size,\n",
    "                               encoderR_size=encoderR_size,\n",
    "                               num_layer=num_layer,                 \n",
    "                               hidden_dim=hidden_dim,\n",
    "                               num_layer_con=num_layer_con,\n",
    "                               hidden_dim_con=hidden_dim_con,\n",
    "                               lr=lr,\n",
    "                               embed_size=embed_size,\n",
    "                               use_glove = use_glove,\n",
    "                               dr=dr,\n",
    "                               dr_con=dr_con,\n",
    "                               memory_dr = memory_dr,\n",
    "                               memory_dim = memory_dim,\n",
    "                               topic_size=topic_size\n",
    "                          )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.build_graph()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#train_model(model, batch_gen, num_train_steps, valid_freq, is_save, graph_prefix)\n",
    "saver = tf.train.Saver()\n",
    "config = tf.ConfigProto()\n",
    "config.gpu_options.allow_growth = True\n",
    "\n",
    "summary = None\n",
    "val_summary = None\n",
    "\n",
    "CAL_ACCURACY_FROM = 1\n",
    "MAX_EARLY_STOP_COUNT = 14\n",
    "\n",
    "with tf.Session(config=config) as sess:\n",
    "    \n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    early_stop_count = MAX_EARLY_STOP_COUNT\n",
    "    \n",
    "    ckpt = tf.train.get_checkpoint_state(os.path.dirname('save/' + graph_dir_name + '/'))\n",
    "    if ckpt and ckpt.model_checkpoint_path:\n",
    "        print ('from check point!!!')\n",
    "        saver.restore(sess, ckpt.model_checkpoint_path)\n",
    "        \n",
    "    writer = tf.summary.FileWriter('./graph/'+graph_dir_name, sess.graph)\n",
    "    \n",
    "    initial_time = time.time()\n",
    "    \n",
    "    max_accr = 0\n",
    "    \n",
    "    for index in range(num_train_steps):\n",
    "    #for index in range(1):\n",
    "    \n",
    "        try:\n",
    "            # run train \n",
    "            \n",
    "            raw_encoder_inputs, raw_encoderR_inputs, raw_encoder_seq, raw_context_seq, raw_encoderR_seq, raw_target_label = batch_gen.get_batch(\n",
    "                                        data=batch_gen.train_set,\n",
    "                                        batch_size=model.batch_size,\n",
    "                                        encoder_size=model.encoder_size,\n",
    "                                        context_size=model.context_size,\n",
    "                                        encoderR_size=model.encoderR_size,\n",
    "                                        is_test=False\n",
    "                                        )\n",
    "            \n",
    "            # prepare data which will be push from pc to placeholder\n",
    "            input_feed = {}\n",
    "            \n",
    "            input_feed[model.encoder_inputs] = raw_encoder_inputs\n",
    "            input_feed[model.encoderR_inputs] = raw_encoderR_inputs\n",
    "\n",
    "            input_feed[model.encoder_seq_length] = raw_encoder_seq\n",
    "            input_feed[model.context_seq_length] = raw_context_seq\n",
    "            input_feed[model.encoderR_seq_length] = raw_encoderR_seq\n",
    "\n",
    "            input_feed[model.y_label] = raw_target_label\n",
    "\n",
    "            input_feed[model.dr_prob] = model.dr\n",
    "            input_feed[model.dr_prob_con] = model.dr_con\n",
    "            input_feed[model.dr_memory_prob] = model.memory_dr\n",
    "            \n",
    "            _, summary, loss = sess.run([model.optimizer, model.summary_op, model.loss], input_feed)\n",
    "            \n",
    "            writer.add_summary( summary, global_step=model.global_step.eval() )\n",
    "            \n",
    "        except:\n",
    "            print(\"excepetion occurs in train step\")\n",
    "            pass\n",
    "        \n",
    "        # run validation\n",
    "        if (index + 1) % valid_freq == 0:\n",
    "            \n",
    "            print('=======')\n",
    "            \n",
    "            num_corr = 0\n",
    "            sum_loss = 0.0\n",
    "            \n",
    "            itr_loop = len(batch_gen.valid_set) / model.batch_size\n",
    "            \n",
    "            for test_itr in range( int(itr_loop) ):\n",
    "                \n",
    "                raw_encoder_inputs, raw_encoderR_inputs, raw_encoder_seq, raw_context_seq, raw_encoderR_seq, raw_target_label = batch_gen.get_batch(\n",
    "                                                                            data=batch_gen.valid_set,\n",
    "                                                                            batch_size=model.batch_size,\n",
    "                                                                            encoder_size=model.encoder_size,\n",
    "                                                                            context_size = model.context_size,\n",
    "                                                                            encoderR_size=model.encoderR_size,\n",
    "                                                                            is_test=True,\n",
    "                                                                            start_index= (test_itr* model.batch_size))\n",
    "                \n",
    "                \n",
    "                # prepare data which will be push from pc to placeholder\n",
    "                input_feed = {}\n",
    "                \n",
    "                input_feed[model.encoder_inputs] = raw_encoder_inputs\n",
    "                input_feed[model.encoderR_inputs] = raw_encoderR_inputs\n",
    "                \n",
    "                input_feed[model.encoder_seq_length] = raw_encoder_seq\n",
    "                input_feed[model.context_seq_length] = raw_context_seq\n",
    "                input_feed[model.encoderR_seq_length] = raw_encoderR_seq\n",
    "                \n",
    "                input_feed[model.y_label] = raw_target_label\n",
    "\n",
    "                input_feed[model.dr_prob] = 1.0          # no drop out while evaluating\n",
    "                input_feed[model.dr_prob_con] = 1.0   # no drop out while evaluating \n",
    "                input_feed[model.dr_memory_prob] = 1.0\n",
    "                \n",
    "                try:\n",
    "                    bprob, b_loss, lo = sess.run([model.batch_prob, model.batch_loss, model.loss], input_feed)\n",
    "                except:\n",
    "                    print(\"excepetion occurs in valid step : {}\".format(str(test_itr)))\n",
    "                    pass\n",
    "                \n",
    "                for idx, prob in enumerate(bprob):\n",
    "                    if prob > 0.5:\n",
    "                        if raw_target_label[idx] == 1:\n",
    "                            num_corr = num_corr + 1\n",
    "                    else:\n",
    "                        if raw_target_label[idx] == 0:\n",
    "                            num_corr = num_corr + 1\n",
    "                            \n",
    "                sum_loss = sum_loss + lo\n",
    "                #print(num_corr / model.batch_size)\n",
    "                \n",
    "            avg_ce = sum_loss / int(itr_loop)\n",
    "            avg_accr = num_corr / ( int(itr_loop) * model.batch_size )\n",
    "            \n",
    "            model.valid_loss = avg_ce\n",
    "            model.accuracy = avg_accr\n",
    "\n",
    "            value1 = summary_pb2.Summary.Value(tag=\"valid_loss\", simple_value=avg_ce)\n",
    "            value2 = summary_pb2.Summary.Value(tag=\"valid_accuracy\", simple_value=avg_accr)\n",
    "            val_summary = summary_pb2.Summary(value=[value1, value2])\n",
    "            \n",
    "            writer.add_summary( val_summary, global_step=model.global_step.eval() )\n",
    "                \n",
    "            end_time = time.time()\n",
    "            \n",
    "            print(avg_ce)\n",
    "            print(avg_accr)\n",
    "            \n",
    "            accr = avg_accr\n",
    "            \n",
    "            if index > CAL_ACCURACY_FROM:\n",
    "                \n",
    "                if ( accr > max_accr ):\n",
    "                    max_accr = accr\n",
    "                    \n",
    "                    # save best result\n",
    "                    if is_save is 1:\n",
    "                        saver.save(sess, 'save/' + graph_dir_name + '/', model.global_step.eval() )\n",
    "\n",
    "                    early_stop_count = MAX_EARLY_STOP_COUNT\n",
    "                    \n",
    "                else:\n",
    "                    # early stopping\n",
    "                    if early_stop_count == 0:\n",
    "                        print(\"early stopped\")\n",
    "                        break\n",
    "                        \n",
    "                    early_stop_count = early_stop_count -1\n",
    "                    \n",
    "    writer.close()\n",
    "    \n",
    "    print ('Total steps : {}'.format(model.global_step.eval()) )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#print(ce)\n",
    "#print(accr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
